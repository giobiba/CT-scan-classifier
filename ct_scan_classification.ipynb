{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ct_scan_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1oFfIlUzPSAE9jcQF6eHsnWpzg6N8I7DG",
      "authorship_tag": "ABX9TyOYEtLXTEt8sG9nC63F3ZAK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giobiba/CT-scan-classifier/blob/main/ct_scan_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQnSbCD1NyZi"
      },
      "source": [
        "import tensorflow as tf\n",
        "import functools\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.image as mpimg\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import zipfile\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "with zipfile.ZipFile(\"./drive/MyDrive/data.zip\", \"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"./data\")\n",
        "\n",
        "\n",
        "# folosim functia functools.partials pentru a simplifica scrierea layerelor mai tarziu in cod\n",
        "Conv2D = functools.partial(tf.keras.layers.Conv2D, activation='relu')\n",
        "MaxPool2D = tf.keras.layers.MaxPool2D\n",
        "Flatten = tf.keras.layers.Flatten\n",
        "Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
        "\n",
        "np.set_printoptions(precision=4, threshold=sys.maxsize)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "VAL_EPOCHS = 3\n",
        "\n",
        "\n",
        "path = \"./data/\"\n",
        "test = \"test\"\n",
        "train = \"train\"\n",
        "validation = \"validation\"\n",
        "width, height = 50, 50\n",
        "n_filters = 24\n",
        "learning_rate = 0.1\n",
        "\n",
        "class PlotLearning(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.metrics = {}\n",
        "        for metric in logs:\n",
        "            self.metrics[metric] = []\n",
        "            \n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        for metric in logs:\n",
        "            if metric in self.metrics:\n",
        "                self.metrics[metric].append(logs.get(metric))\n",
        "            else:\n",
        "                self.metrics[metric] = [logs.get(metric)]\n",
        "\n",
        "        metrics = [x for x in logs if 'val' not in x]\n",
        "        \n",
        "        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        for i, metric in enumerate(metrics):\n",
        "            axs[i].plot(range(1, epoch + 2), \n",
        "                        self.metrics[metric], \n",
        "                        label=metric)\n",
        "            if logs['val_' + metric]:\n",
        "                axs[i].plot(range(1, epoch + 2), \n",
        "                            self.metrics['val_' + metric], \n",
        "                            label='val_' + metric)\n",
        "                \n",
        "            axs[i].legend()\n",
        "            axs[i].grid()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def get_images_and_labels(file = \"train\"):\n",
        "  images = []\n",
        "  labels = []\n",
        "  with open(path + file + \".txt\") as f:\n",
        "    for line in f.readlines():\n",
        "      img_path, label = line.split(\",\")\n",
        "      img = mpimg.imread(path + file + \"/\" + img_path)\n",
        "      img = np.expand_dims(img, axis = -1)\n",
        "      label = int(label)\n",
        "      images.append(img) \n",
        "      labels.append(label)\n",
        "      \n",
        "  return np.array(images), np.array(labels)\n",
        "\n",
        "def get_images(file = \"test\"):\n",
        "  images = []\n",
        "  paths = []\n",
        "  with open(path + file + \".txt\") as f:\n",
        "    for line in f.readlines():\n",
        "      img_path = line.strip()\n",
        "      img = mpimg.imread(path + file + \"/\" + img_path)\n",
        "      img = np.expand_dims(img, axis = -1)\n",
        "      images.append(img) \n",
        "      paths.append(img_path)\n",
        "  return np.array(images), paths\n",
        "\n",
        "train_images, train_labels = get_images_and_labels(train)\n",
        "val_images, val_labels = get_images_and_labels(validation)\n",
        "\n",
        "test_images, test_paths = get_images(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycipRBtqqQAZ"
      },
      "source": [
        "Unziping the files that we will train out CNN on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeHXUR-G-Rpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a4a4f18-5cbd-45b3-89f8-ff11c5eaf156"
      },
      "source": [
        "def build_model(width, height):\n",
        "  global n_filters\n",
        "  model = tf.keras.Sequential(\n",
        "      layers=[\n",
        "        Conv2D(filters=1*n_filters, kernel_size=(3,3), strides=2, input_shape = (height, width, 1)),\n",
        "        MaxPool2D((2,2)),\n",
        "\n",
        "        Conv2D(filters=2*n_filters, kernel_size=(3,3)),\n",
        "        MaxPool2D((2,2)),\n",
        "\n",
        "        Conv2D(filters=4*n_filters, kernel_size=(3,3)),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(512),\n",
        "        Dense(3, activation=\"softmax\")\n",
        "      ])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "model = build_model(height, width)\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.02), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 24, 24, 24)        240       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 12, 12, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 10, 10, 48)        10416     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 48)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 96)          41568     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 864)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               442880    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 496,643\n",
            "Trainable params: 496,643\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL4PIfhh-yUo"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aThUgNsN3Zj"
      },
      "source": [
        "# training the model with the train set\n",
        "history = model.fit(x = train_images, \n",
        "                    y = train_labels, \n",
        "                    batch_size=BATCH_SIZE, \n",
        "                    epochs=EPOCHS, \n",
        "                    shuffle=True, \n",
        "                    use_multiprocessing=True,\n",
        "                    validation_data = (val_images, val_labels),\n",
        "                    callbacks=[PlotLearning()])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-igJN7oeXbj1"
      },
      "source": [
        "# antrenam modelul si pe datele\n",
        "history = model.fit(x = val_images, \n",
        "                    y = val_labels, \n",
        "                    batch_size=BATCH_SIZE, \n",
        "                    epochs=VAL_EPOCHS, \n",
        "                    shuffle=True, \n",
        "                    use_multiprocessing=True,\n",
        "                    callbacks=[PlotLearning])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f96El5J_-uzI"
      },
      "source": [
        "Evaluating our trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Or4HhpAPV6P",
        "outputId": "7762a497-5e02-44e1-a62c-3fd8c4ff33ab"
      },
      "source": [
        "val_loss, val_acc = model.evaluate(val_images, val_labels)\n",
        "print(\"Model's accuracy: {}%\".format(round(val_acc * 100, 4)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "141/141 [==============================] - 0s 3ms/step - loss: 0.7184 - accuracy: 0.6816\n",
            "Model's accuracy: 68.1556%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oomko7elYIX8",
        "outputId": "885e835f-960f-4f6a-b0ff-501cc702e3ad"
      },
      "source": [
        "y_pred = model.predict(val_images)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(val_labels, y_pred))\n",
        "print(\"Classification report\")\n",
        "print(classification_report(val_labels, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "[[1289  174   37]\n",
            " [ 406  649  445]\n",
            " [ 285  225  990]]\n",
            "Classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.86      0.74      1500\n",
            "           1       0.62      0.43      0.51      1500\n",
            "           2       0.67      0.66      0.67      1500\n",
            "\n",
            "    accuracy                           0.65      4500\n",
            "   macro avg       0.65      0.65      0.64      4500\n",
            "weighted avg       0.65      0.65      0.64      4500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BvdYSc9-fk5"
      },
      "source": [
        "  Predicting the classes for the images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKMB32vlmZk7"
      },
      "source": [
        "predicted_labels = model.predict(test_images)\n",
        "predicted = list(map(lambda x: np.argmax(x), predicted_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jinvCoYU-q9t"
      },
      "source": [
        "Write the result from our predictions to a csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSNM82lAn7E1"
      },
      "source": [
        "with open(\"./drive/MyDrive/results.csv\", \"w\") as f:\n",
        "  f.write(\"id,label\\n\")\n",
        "  for i in range(len(predicted_labels)):\n",
        "    f.write(\"{},{}\\n\".format(test_paths[i], predicted[i]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}